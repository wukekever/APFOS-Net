{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_width, hidden_num, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_in = nn.Linear(input_size, hidden_width)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_width, hidden_width) for _ in range(hidden_num)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_width, output_size)\n",
    "\n",
    "    def forward(self, xz):\n",
    "        output = self.layer_in(xz)\n",
    "        for i, h_i in enumerate(self.hidden_layers):\n",
    "            output = self.activation(h_i(output))\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "\n",
    "    def activation(self, o):\n",
    "        return torch.tanh(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c910506",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dimension = 2\n",
    "d = space_dimension\n",
    "input_size = d \n",
    "hidden_width = 128\n",
    "hidden_num = 4\n",
    "output_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6eee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net(input_size, hidden_width, hidden_num, output_size)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier normal initialization for weights:\n",
    "#             mean = 0 std = gain * sqrt(2 / fan_in + fan_out)\n",
    "# zero initialization for biases\n",
    "def Xavier_initi(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m,torch.nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a396b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_param(net, path):\n",
    "    torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xavier_initi(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4641ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xz):\n",
    "    model_phi = net(xz)[:, 0:1]\n",
    "    model_tau = net(xz)[:, 1:2] \n",
    "    model_sigma = net(xz)[:, 2:3] \n",
    "    temp = torch.cat((model_phi, model_tau), 1)\n",
    "    model_temp = torch.cat((temp, model_sigma), 1)\n",
    "    return model_temp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sample\n",
    "# data: [0, 1] \\times [0, 1]\n",
    "def generate_sample(data_size_temp):\n",
    "    X, Z = 1.0, 1.0\n",
    "    x_test = np.linspace(0, X, data_size_temp).reshape(data_size_temp, 1)\n",
    "    z_test = np.linspace(0, Z, data_size_temp).reshape(data_size_temp, 1)\n",
    "    x_test, z_test = np.meshgrid(x_test, z_test)\n",
    "    X_test = x_test.reshape(x_test.shape[0] * x_test.shape[1], 1)\n",
    "    Z_test = z_test.reshape(z_test.shape[0] * z_test.shape[1], 1)\n",
    "    xz_temp = np.concatenate((X_test, Z_test), axis=1)\n",
    "    sample_temp = torch.tensor(xz_temp).float()\n",
    "    return sample_temp.to(device)\n",
    "\n",
    "\n",
    "# data: {0} \\times [0, 1]\n",
    "def generate_sample_bdy_left(boundary_data_size_temp):\n",
    "    X, Z = 1.0, 1.0\n",
    "    x_test = np.zeros((boundary_data_size_temp, 1))\n",
    "    z_test = np.linspace(0, Z, boundary_data_size_temp).reshape(\n",
    "        boundary_data_size_temp, 1\n",
    "    )\n",
    "    xz_temp = np.concatenate((x_test, z_test), axis=1)\n",
    "    sample_temp = torch.tensor(xz_temp).float()\n",
    "    return sample_temp.to(device)\n",
    "\n",
    "\n",
    "# data: {1} \\times [0, 1]\n",
    "def generate_sample_bdy_right(boundary_data_size_temp):\n",
    "    X, Z = 1.0, 1.0\n",
    "    x_test = np.ones((boundary_data_size_temp, 1)) * X\n",
    "    z_test = np.linspace(0, Z, boundary_data_size_temp).reshape(\n",
    "        boundary_data_size_temp, 1\n",
    "    )\n",
    "    xz_temp = np.concatenate((x_test, z_test), axis=1)\n",
    "    sample_temp = torch.tensor(xz_temp).float()\n",
    "    return sample_temp.to(device)\n",
    "\n",
    "\n",
    "# data: [0, 1] \\times {0}\n",
    "def generate_sample_bdy_down(boundary_data_size_temp):\n",
    "    X, Z = 1.0, 1.0\n",
    "    x_test = np.linspace(0, X, boundary_data_size_temp).reshape(\n",
    "        boundary_data_size_temp, 1\n",
    "    )\n",
    "    z_test = np.zeros((boundary_data_size_temp, 1))\n",
    "    xz_temp = np.concatenate((x_test, z_test), axis=1)\n",
    "    sample_temp = torch.tensor(xz_temp).float()\n",
    "    return sample_temp.to(device)\n",
    "\n",
    "\n",
    "# data: [0, 1] \\times {1}\n",
    "def generate_sample_bdy_up(boundary_data_size_temp):\n",
    "    X, Z = 1.0, 1.0\n",
    "    x_test = np.linspace(0, X, boundary_data_size_temp).reshape(\n",
    "        boundary_data_size_temp, 1\n",
    "    )\n",
    "    z_test = np.ones((boundary_data_size_temp, 1)) * Z\n",
    "    xz_temp = np.concatenate((x_test, z_test), axis=1)\n",
    "    sample_temp = torch.tensor(xz_temp).float()\n",
    "    return sample_temp.to(device)\n",
    "\n",
    "\n",
    "def b(xz, theta, m, w):\n",
    "    (x, z) = torch.split(xz, 1, dim=1)\n",
    "    B_x = m * np.pi * theta * (x**2 - x) * torch.sin(m * np.pi * z)\n",
    "    B_z = np.pi + theta * (2 * x - 1.0) * torch.cos(m * np.pi * z)\n",
    "    B_field = torch.cat((B_x, B_z), 1)\n",
    "    B_norm = torch.norm(B_field, p=2, dim=1, keepdim=True)\n",
    "    return (B_field / B_norm).to(device)\n",
    "\n",
    "\n",
    "def b_orthogonal(xz, theta, m, w):\n",
    "    b_temp = b(xz, theta, m, w)\n",
    "    (b_x, b_z) = torch.split(b_temp, 1, dim=1)\n",
    "    orthogonal = torch.cat((-b_z, b_x), 1)\n",
    "    return orthogonal.to(device)\n",
    "\n",
    "\n",
    "def exact_solution(xz, theta, m, w, eps):\n",
    "    (x, z) = torch.split(xz, 1, dim=1)\n",
    "    phi_0 = torch.sin(w * (np.pi * x + theta * (x**2 - x) * torch.cos(m * np.pi * z)))\n",
    "    phi_temp = phi_0 + eps * torch.cos(2 * np.pi * z) * torch.sin(np.pi * x)\n",
    "    return phi_temp.to(device)\n",
    "\n",
    "\n",
    "def tensor_dot(tensor_x, tensor_y):\n",
    "    dot_product = tensor_x * tensor_y\n",
    "    return torch.sum(dot_product, dim=1, keepdim=True).to(device)\n",
    "\n",
    "\n",
    "def f(xz, theta, m, w, eps):\n",
    "    #      xz.requires_grad = True\n",
    "    (x, z) = torch.split(xz, 1, dim=1)\n",
    "    xz = torch.cat((x, z), 1)\n",
    "    b_case = b(xz, theta, m, w)\n",
    "    b_orth_case = b_orthogonal(xz, theta, m, w)\n",
    "\n",
    "    solu = exact_solution(xz, theta, m, w, eps)\n",
    "\n",
    "    gradient_solu = torch.autograd.grad(\n",
    "        outputs=solu,\n",
    "        inputs=xz,\n",
    "        grad_outputs=torch.ones(solu.shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    operator_para = tensor_dot(gradient_solu, b_case) * b_case\n",
    "    operator_perp = tensor_dot(gradient_solu, b_orth_case) * b_orth_case\n",
    "\n",
    "    operator_para_x = torch.autograd.grad(\n",
    "        outputs=operator_para[:, 0:1],\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones(operator_para[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    operator_para_z = torch.autograd.grad(\n",
    "        outputs=operator_para[:, 1:2],\n",
    "        inputs=z,\n",
    "        grad_outputs=torch.ones(operator_para[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    operator_perp_x = torch.autograd.grad(\n",
    "        outputs=operator_perp[:, 0:1],\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones(operator_perp[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    operator_perp_z = torch.autograd.grad(\n",
    "        outputs=operator_perp[:, 1:2],\n",
    "        inputs=z,\n",
    "        grad_outputs=torch.ones(operator_perp[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    divergence_para = operator_para_x + operator_para_z\n",
    "    divergence_perp = operator_perp_x + operator_perp_z\n",
    "\n",
    "    f_temp = -divergence_perp - 1.0 / eps * divergence_para\n",
    "    return f_temp.to(device)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def error_function(xz, theta, m, w, eps):\n",
    "    solu_ex = exact_solution(xz, theta, m, w, eps)\n",
    "    solu_pred = model(xz)[:, 0:1]\n",
    "    residual = solu_ex - solu_pred\n",
    "    relative_error = torch.sum(residual**2) / torch.sum(solu_ex**2)\n",
    "    return relative_error.to(device)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def loss_function(xz, xz_left, xz_right, xz_down, xz_up, theta, m, w, eps):\n",
    "\n",
    "    (x, z) = torch.split(xz, 1, dim=1)\n",
    "    xz = torch.cat((x, z), 1)\n",
    "\n",
    "    model_hat = model(xz)\n",
    "    phi_hat = model_hat[:, 0:1]\n",
    "    tau_hat = model_hat[:, 1:2]\n",
    "    sigma_hat = model_hat[:, 2:3]\n",
    "\n",
    "    b_case = b(xz, theta, m, w)\n",
    "    b_orth_case = b_orthogonal(xz, theta, m, w)\n",
    "\n",
    "    multi_1_hat = tau_hat * b_orth_case  # tau * b_orthogonal\n",
    "    multi_2_hat = sigma_hat * b_case  # sigma * b\n",
    "\n",
    "    gradient_phi_hat = torch.autograd.grad(\n",
    "        outputs=phi_hat,\n",
    "        inputs=xz,\n",
    "        grad_outputs=torch.ones(phi_hat.shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    dot_1_hat = tensor_dot(\n",
    "        gradient_phi_hat, b_orth_case\n",
    "    )  # inner product of gradient_phi and b_orthogonal\n",
    "    dot_2_hat = tensor_dot(\n",
    "        gradient_phi_hat, b_case\n",
    "    )  # inner product of gradient_phi and b\n",
    "\n",
    "    multi_1_x_hat = torch.autograd.grad(\n",
    "        outputs=multi_1_hat[:, 0:1],\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones(multi_1_hat[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    multi_1_z_hat = torch.autograd.grad(\n",
    "        outputs=multi_1_hat[:, 1:2],\n",
    "        inputs=z,\n",
    "        grad_outputs=torch.ones(multi_1_hat[:, 1:2].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    divergence_1 = multi_1_x_hat + multi_1_z_hat  # divergence of tau * b_orthogonal\n",
    "\n",
    "    multi_2_x_hat = torch.autograd.grad(\n",
    "        outputs=multi_2_hat[:, 0:1],\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones(multi_2_hat[:, 0:1].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    multi_2_z_hat = torch.autograd.grad(\n",
    "        outputs=multi_2_hat[:, 1:2],\n",
    "        inputs=z,\n",
    "        grad_outputs=torch.ones(multi_2_hat[:, 1:2].shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "    divergence_2 = multi_2_x_hat + multi_2_z_hat  # divergence of sigma * b\n",
    "\n",
    "    part_1 = torch.sum(\n",
    "        (divergence_1 + divergence_2 + f(xz, theta, m, w, eps)) ** 2\n",
    "    ) / len(xz)\n",
    "\n",
    "    part_2 = torch.sum((dot_1_hat - tau_hat) ** 2) / len(xz)\n",
    "\n",
    "    part_3 = torch.sum((dot_2_hat - eps * sigma_hat) ** 2) / len(xz)\n",
    "\n",
    "    # Boundary - Gamma_D\n",
    "    (x_left, z_left) = torch.split(xz_left, 1, dim=1)\n",
    "    xz_left = torch.cat((x_left, z_left), 1)\n",
    "    model_left_hat = model(xz_left)\n",
    "    phi_left_hat = model_left_hat[:, 0:1]\n",
    "\n",
    "    (x_right, z_right) = torch.split(xz_right, 1, dim=1)\n",
    "    xz_right = torch.cat((x_right, z_right), 1)\n",
    "    model_right_hat = model(xz_right)\n",
    "    phi_right_hat = model_right_hat[:, 0:1]\n",
    "\n",
    "    part_4 = (\n",
    "        torch.sum((phi_left_hat - exact_solution(xz_left, theta, m, w, eps)) ** 2)\n",
    "        / len(xz_left)\n",
    "        / 2.0\n",
    "        + torch.sum((phi_right_hat - exact_solution(xz_right, theta, m, w, eps)) ** 2)\n",
    "        / len(xz_right)\n",
    "        / 2.0\n",
    "    )\n",
    "\n",
    "    # Boundary - Gamma_N\n",
    "    (x_down, z_down) = torch.split(xz_down, 1, dim=1)\n",
    "    xz_down = torch.cat((x_down, z_down), 1)\n",
    "    model_down_hat = model(xz_down)\n",
    "    phi_down_hat = model_down_hat[:, 0:1]\n",
    "    tau_down_hat = model_down_hat[:, 1:2]\n",
    "    sigma_down_hat = model_down_hat[:, 2:3]\n",
    "\n",
    "    gradient_phi_down_hat = torch.autograd.grad(\n",
    "        outputs=phi_down_hat,\n",
    "        inputs=xz_down,\n",
    "        grad_outputs=torch.ones(phi_down_hat.shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    b_down_case = b(xz_down, theta, m, w)\n",
    "    b_down_orth_case = b_orthogonal(xz_down, theta, m, w)\n",
    "\n",
    "    operator_down_para = tensor_dot(gradient_phi_down_hat, b_down_case) * b_down_case\n",
    "    operator_down_perp = (\n",
    "        tensor_dot(gradient_phi_down_hat, b_down_orth_case) * b_down_orth_case\n",
    "    )\n",
    "\n",
    "    (x_up, z_up) = torch.split(xz_up, 1, dim=1)\n",
    "    xz_up = torch.cat((x_up, z_up), 1)\n",
    "    model_up_hat = model(xz_up)\n",
    "    phi_up_hat = model_up_hat[:, 0:1]\n",
    "    tau_up_hat = model_up_hat[:, 1:2]\n",
    "    sigma_up_hat = model_up_hat[:, 2:3]\n",
    "\n",
    "    gradient_phi_up_hat = torch.autograd.grad(\n",
    "        outputs=phi_up_hat,\n",
    "        inputs=xz_up,\n",
    "        grad_outputs=torch.ones(phi_up_hat.shape).to(device),\n",
    "        create_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    b_up_case = b(xz_up, theta, m, w)\n",
    "    b_up_orth_case = b_orthogonal(xz_up, theta, m, w)\n",
    "\n",
    "    operator_up_para = tensor_dot(gradient_phi_up_hat, b_up_case) * b_up_case\n",
    "    operator_up_perp = tensor_dot(gradient_phi_up_hat, b_up_orth_case) * b_up_orth_case\n",
    "\n",
    "    part_5 = (\n",
    "        torch.sum(\n",
    "            (\n",
    "                operator_down_para[:, 1:2] * (-1.0)\n",
    "                + eps * operator_down_perp[:, 1:2] * (-1.0)\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        / len(xz_down)\n",
    "        / 2.0\n",
    "        + torch.sum(\n",
    "            (operator_up_para[:, 1:2] * (1.0) + eps * operator_up_perp[:, 1:2] * (1.0))\n",
    "            ** 2\n",
    "        )\n",
    "        / len(xz_up)\n",
    "        / 2.0\n",
    "    )\n",
    "\n",
    "    part_6 = (\n",
    "        torch.sum(\n",
    "            (\n",
    "                b_down_orth_case[:, 1:2] * (-1.0) * tau_down_hat\n",
    "                + b_down_case[:, 1:2] * (-1.0) * sigma_down_hat\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        / len(xz_down)\n",
    "        / 2.0\n",
    "        + torch.sum(\n",
    "            (\n",
    "                b_up_orth_case[:, 1:2] * (1.0) * tau_up_hat\n",
    "                + b_down_case[:, 1:2] * (1.0) * sigma_up_hat\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        / len(xz_up)\n",
    "        / 2.0\n",
    "    )\n",
    "\n",
    "    beta_D, beta_N = 1.0, 1.0\n",
    "    summation = part_1 + part_2 + part_3 + beta_D * part_4 + beta_N * (part_5 + part_6)\n",
    "\n",
    "    return summation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer and learning rate decay\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 1000, 0.9) # every 5000 epoch, learning rate * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iter = 2000\n",
    "theta, m, w, eps = 0, 1, 2, 1\n",
    "xz = generate_sample(data_size_temp=100)\n",
    "xz_left = generate_sample_bdy_left(boundary_data_size_temp=100)\n",
    "xz_right = generate_sample_bdy_right(boundary_data_size_temp=100)\n",
    "xz_down = generate_sample_bdy_down(boundary_data_size_temp=100)\n",
    "xz_up = generate_sample_bdy_up(boundary_data_size_temp=100)\n",
    "xz.requires_grad = True\n",
    "xz_left.requires_grad = True\n",
    "xz_right.requires_grad = True\n",
    "xz_down.requires_grad = True\n",
    "xz_up.requires_grad = True\n",
    "\n",
    "loss_record = np.zeros(Iter)\n",
    "error_record = np.zeros(Iter)\n",
    "time_start = time.time()\n",
    "for it in range(Iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(xz, xz_left, xz_right, xz_down, xz_up, theta, m, w, eps)\n",
    "    error = error_function(xz, theta, m, w, eps)\n",
    "    loss_record[it] = float(loss)\n",
    "    error_record[it] = float(error)\n",
    "    if it % 1 == 0:\n",
    "        print(\"\")\n",
    "        print(\n",
    "            \"[Iteration step: {}/{} - Loss: {:.2e} - Error: {:.2e}]\".format(\n",
    "                it + 1, Iter, loss.detach(), error.detach()\n",
    "            )\n",
    "        )\n",
    "        print(\"\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "np.save(\"loss_uniform_case_1.npy\", loss_record)\n",
    "np.save(\"error_uniform_case_1.npy\", error_record)\n",
    "time_end = time.time()\n",
    "print(\"Total time for training is: \", time_end - time_start, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbe513",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_function(xz, theta, m, w, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_param(net, path = './net_params_case_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab75ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(x, z, theta, m, w, eps):\n",
    "    phi_0 = np.sin(w * (np.pi*x + theta*(x**2-x)*np.cos(m*np.pi*z)))\n",
    "    phi_temp = phi_0 + eps * np.cos(2*np.pi*z) * np.sin(np.pi*x)\n",
    "    return phi_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "step = 0.01\n",
    "x = np.arange(0.0, 1.0, step)\n",
    "z = np.arange(0.0, 1.0, step)\n",
    "# meshgrid\n",
    "X, Z = np.meshgrid(x, z)\n",
    "# function w.r.t. x and z \n",
    "ex_solu = exact_solution(X, Z, theta, m, w, eps)\n",
    "\n",
    "# set figure: length = 10 width = 6\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# fill color\n",
    "cset = plt.contourf(X, Z, ex_solu, 6, cmap = 'rainbow')\n",
    "# plt.contourf(X,Y,Z, cmap = 'hot')\n",
    "# or cmap = plt.cm.hot\n",
    "\n",
    "contour = plt.contour(X, Z, ex_solu, 8, colors = 'k')\n",
    "\n",
    "# label\n",
    "plt.clabel(contour, fontsize = 10, colors = 'k')\n",
    " \n",
    "# show bar\n",
    "plt.colorbar(cset)\n",
    "plt.xlabel(r\"$x$\",size = 12) \n",
    "plt.ylabel(r\"$z$\",size = 12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d629e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = np.zeros((100, 100))\n",
    "dx = np.zeros((1, 2))\n",
    "dx[:, 0] = 1.0\n",
    "dz = np.zeros((1, 2))\n",
    "dz[:, 1] = 1.0\n",
    "for j in range(100):\n",
    "    zj = 0.0 + j * 0.01\n",
    "    for i in range(100):\n",
    "        xi = 0.0 + i * 0.01\n",
    "        x_z = xi*dx + zj*dz\n",
    "        tensor_xz = torch.from_numpy(x_z)\n",
    "        approx[i, j] = net(tensor_xz.to(torch.float32))[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f99056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# set figure: length = 10 width = 6\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# fill color\n",
    "cset = plt.contourf(Z, X, approx, 6, cmap = 'rainbow')\n",
    "# plt.contourf(X,Y,Z, cmap = 'hot')\n",
    "# or cmap = plt.cm.hot\n",
    "\n",
    "contour = plt.contour(Z, X, approx, 8, colors = 'k')\n",
    "\n",
    "# label\n",
    "plt.clabel(contour, fontsize = 10, colors = 'k')\n",
    " \n",
    "# show bar\n",
    "plt.colorbar(cset)\n",
    "plt.xlabel(r\"$x$\",size = 12) \n",
    "plt.ylabel(r\"$z$\",size = 12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abbd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b2d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bda8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
